{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "colab": {
   "name": "rnn_error_analysis.ipynb",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpjUssRKXJB6",
    "colab_type": "text"
   },
   "source": [
    "# Error Analisys using RNN's on original Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoGuhC5cXJB8",
    "colab_type": "text"
   },
   "source": [
    "### First we're going to tune up a simple rnn to analize the generalization error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13OcyabhXJB_",
    "colab_type": "text"
   },
   "source": [
    "### Setting up variables, library and data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_Xpi46BAX3dO",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Run this cell to mount your Google Drive.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6N5p3MiREQza",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def load_data(label):\n",
    "  #label = 'all'\n",
    "  print(\"Loading data with all components\")\n",
    "  names_X = ['X_train_labels_niveladas.csv', 'X_test.csv']\n",
    "  names_y = ['y_train_labels_niveladas.csv', 'y_test.csv']\n",
    "\n",
    "  X = []\n",
    "  y = []\n",
    "\n",
    "  for dataset in names_X:\n",
    "    dataframe = pd.read_csv(''.join([path,dataset]), index_col=0)\n",
    "    dataframe = dataframe.iloc[:, dataframe.columns.str.contains(features)]\n",
    "    #X.append(np.array(dataframe))\n",
    "    X.append(dataframe)\n",
    "\n",
    "  for dataset in names_y:\n",
    "    dataframe = pd.read_csv(''.join([path,dataset]),index_col=0)\n",
    "    #y.append(np.array(dataframe))\n",
    "    y.append(dataframe)\n",
    "\n",
    "  print('Shape X_train: ', np.shape(X[0]))\n",
    "  print('Shape X_test : ', np.shape(X[1]))\n",
    "  print('Shape y_train: ', np.shape(y[0]))\n",
    "  print('Shape y_test : ', np.shape(y[1]))\n",
    "\n",
    "  #return X_train, X_test, y_train, y_test\n",
    "  return X[0], X[1], y[0], y[1], features # X_train, X_test, y_train, y_test"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZZWm6lxNtOc1",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def reshape_for_lstm(X_reshape):\n",
    "  print(\"Reshaping for LSTM\")\n",
    "  X_reshape = np.array(X_reshape)\n",
    "  #X_reshape = np.array(X_train)[:,:-1]\n",
    "  number_of_features = parameters.count('|') + 1\n",
    "\n",
    "  shape_input = int(np.shape(X_reshape)[1]/number_of_features)\n",
    "  X_new = np.array([])\n",
    "\n",
    "  for example in X_reshape:\n",
    "    # split data for each component, i.e., [fx, fy, fz]\n",
    "    X = np.split(example, number_of_features)\n",
    "    # reshapes each component for LSTM shape\n",
    "    for i, x in enumerate(X):\n",
    "      X[i] = np.reshape(x, (shape_input, 1))\n",
    "\n",
    "    # concatenate all components with new shape and transpose it to be in (n_experiment, timesteps, components)\n",
    "    X_example = np.concatenate([[x for x in X]])\n",
    "    X_example = np.transpose(X_example)\n",
    "    if X_new.shape == (0,):\n",
    "      X_new = X_example\n",
    "    else:\n",
    "      X_new = np.concatenate((X_new, X_example))\n",
    "\n",
    "  print(\"X_new.shape = \", X_new.shape)\n",
    "  #print(\"new y data shape = \", X_test_full.shape)\n",
    "\n",
    "  return X_new"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RTxEXZLkXJCC",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "## Data path handlers\n",
    "#TRAIN_TEST_SET_PATH = '../../Data/train_test/'\n",
    "path = r'/content/drive/My Drive/USP/Doutorado/Artigos - Escrita/2020 - IROS/Datasets/SMOTe_Nivelado/all_components/'\n",
    "META_DATA_PATH = r'/content/drive/My Drive/USP/Doutorado/Artigos - Escrita/2020 - IROS/Datasets/SMOTe_Nivelado/all_components/glahr_meta/'\n",
    "AUG_SET_PATH = '../../Data/aug_data_all/'\n",
    "MODEL_PATH = r'/content/drive/My Drive/USP/Doutorado/Artigos - Escrita/2020 - IROS/Datasets/SMOTe_Nivelado/all_components/glahr_models/'\n",
    "IMAGES_PATH = r'/content/drive/My Drive/USP/Doutorado/Artigos - Escrita/2020 - IROS/Datasets/SMOTe_Nivelado/all_components/glahr_images/'\n",
    "\n",
    "## Selecting the desired features\n",
    "parameters = 'fx|fy|fz|mx|my|mz'"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nWq0xY9sXJCP",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "### Getting data and selecting features\n",
    "#X_train = pd.read_csv(TRAIN_TEST_SET_PATH+'X_train.csv',index_col=0)\n",
    "#y_train = pd.read_csv(TRAIN_TEST_SET_PATH+'y_train.csv',index_col=0)\n",
    "#X_test = pd.read_csv(TRAIN_TEST_SET_PATH+'X_test.csv',index_col=0)\n",
    "#y_test = pd.read_csv(TRAIN_TEST_SET_PATH+'y_test.csv',index_col=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test, features = load_data(parameters)\n",
    "X_train['labels'] = y_train.copy()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xMPwjgMCXJCf",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "### Creating the validation set\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=27)\n",
    "#for train, val in split.split(X_train, X_train['labels']):\n",
    "for train, val in split.split(X_train, X_train['labels']):\n",
    "    X_train_vl = X_train.iloc[train].copy()\n",
    "    X_val = X_train.iloc[val].copy()\n",
    "    \n",
    "y_train_vl = X_train_vl['labels'].copy()\n",
    "y_val = X_val['labels'].copy()\n",
    "\n",
    "X_train_vl = X_train_vl.iloc[:, ~X_train_vl.columns.str.contains('labels')]\n",
    "X_val = X_val.iloc[:, ~X_val.columns.str.contains('labels')]\n",
    "#X_train = X_train.iloc[:, ~X_train.columns.str.contains('labels')]\n",
    "\n",
    "y_train_vl = np.array(y_train_vl)-1\n",
    "y_val = np.array(y_val)-1\n",
    "y_test = np.array(y_test)-1\n",
    "y_train = np.array(y_train)-1\n",
    "\n",
    "print(\"X_train_vl.shape = \", X_train_vl.shape)\n",
    "print(\"X_val.shape = \", X_val.shape)\n",
    "\n",
    "print(\"y_train_vl.shape = \", y_train_vl.shape)\n",
    "print(\"y_val.shape = \", y_val.shape)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "I8RHwNi4XJCm",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "### Standardizing the data\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#std_scaler = StandardScaler()\n",
    "\n",
    "#X_train = std_scaler.fit_transform(X_train)\n",
    "#X_test = std_scaler.transform(X_test)\n",
    "#X_train_vl = std_scaler.transform(X_train_vl)\n",
    "#X_val = std_scaler.transform(X_val)\n",
    "\n",
    "# AJUSTAR AQUI\n",
    "X_train = X_train/30\n",
    "X_test = X_test/30\n",
    "X_train_vl = X_train_vl/30\n",
    "X_val = X_val/30\n",
    "print(\"X_train.shape = \", X_train.shape)\n",
    "print(\"X_test.shape = \", X_test.shape)\n",
    "print(\"X_train_vl.shape = \", X_train_vl.shape)\n",
    "print(\"X_val.shape = \", X_val.shape)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WYKeavrDGpAe",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "### Reshaping data for LSTM\n",
    "X_train = reshape_for_lstm(np.array(X_train)[:,:-1])\n",
    "X_test = reshape_for_lstm(X_test)\n",
    "X_train_vl = reshape_for_lstm(X_train_vl)\n",
    "X_val = reshape_for_lstm(X_val)\n",
    "\n",
    "print(\"\\n\\n\\nX_train.shape = \", X_train.shape)\n",
    "print(\"X_test.shape = \", X_test.shape)\n",
    "print(\"X_train_vl.shape = \", X_train_vl.shape)\n",
    "print(\"X_val.shape = \", X_val.shape)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Liy2NzRfXJCt",
    "colab_type": "text"
   },
   "source": [
    "### Setting up the mlp model and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V9SO3BGkXJCv",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "### Training a simple rnn to use as reference\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7ro4tEEDXJC1",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "'''\n",
    "### Model sketch\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[1556]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation='relu'))\n",
    "    model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss='sparse_categorical_crossentropy'\n",
    "                 ,optimizer=optimizer\n",
    "                 ,metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "mlp_simple = keras.wrappers.scikit_learn.KerasClassifier(build_model)\n",
    "'''"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "k4nfDSpphqQ9",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "### Model LSTM sketch\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=1556, features=6, output_shape=3, dropout_com=0.0, dropout_rec=0.0):\n",
    "\n",
    "  model = keras.models.Sequential()\n",
    "  # input layer\n",
    "  if n_hidden == 0:\n",
    "    model.add(keras.layers.LSTM(units=n_neurons, input_shape=(input_shape,features), return_sequences=False, dropout=dropout_com))\n",
    "  else:\n",
    "    model.add(keras.layers.LSTM(units=n_neurons, input_shape=(input_shape,features), return_sequences=True, dropout=dropout_com, recurrent_dropout=dropout_rec))\n",
    "    if n_hidden > 1:\n",
    "      for layer in range(n_hidden-1):\n",
    "        model.add(keras.layers.LSTM(units=n_neurons, return_sequences=True, dropout=dropout_com, recurrent_dropout=dropout_rec))\n",
    "      else:\n",
    "        model.add(keras.layers.LSTM(units=n_neurons, return_sequences=False, dropout=dropout_com))\n",
    "\n",
    "  # output layer\n",
    "  model.add(keras.layers.Dense(output_shape, activation='softmax'))\n",
    "  optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "  model.compile(loss='sparse_categorical_crossentropy'\n",
    "                ,optimizer=optimizer\n",
    "                ,metrics=['accuracy'])\n",
    "  print(model.summary())\n",
    "  return model\n",
    "\n",
    "lstm_model = keras.wrappers.scikit_learn.KerasClassifier(build_model)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bjuTVquXXJC6",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "### Setting Callbacks\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=15,\n",
    "                                               restore_best_weights=True)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(MODEL_PATH+\"lstm_simple.h5\",\n",
    "                                                save_best_only=True)                                     \n",
    "\n",
    "get_stats = pd.DataFrame([])\n",
    "get_stats.to_csv(META_DATA_PATH+'lstm_error_analysis.csv')\n",
    "\n",
    "class GetLossAnalysis(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        get_stats = pd.read_csv(META_DATA_PATH+'lstm_error_analysis.csv', index_col=0)\n",
    "        \n",
    "        if len(get_stats.columns) == 0:\n",
    "            get_stats.columns = ['epoch', 'train_loss', 'test_loss' ]\n",
    "            \n",
    "        get_stats['epoch'] = epoch\n",
    "        get_stats['train_loss'] = logs.get('loss')\n",
    "        get_stats['test_loss'] = logs.get('val_loss')\n",
    "        \n",
    "        get_stats.to_csv(META_DATA_PATH+'lstm_error_analysis.csv')\n",
    "\n",
    "loss_analisys_cb = GetLossAnalysis()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DEdzXPfXJDA",
    "colab_type": "text"
   },
   "source": [
    "### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9smvGW2-XJDB",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "### Setting parameters range for hyperparameter optimzation\n",
    "### Here we will use random search as the optimization method\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "params = {\n",
    "    'n_hidden': np.arange(1,5),\n",
    "    'n_neurons': np.arange(1,32),\n",
    "    'learning_rate': reciprocal(3e-4, 3e-2),\n",
    "    'dropout_com': np.arange(0,6)/10,\n",
    "    'dropout_rec': np.arange(0,6)/10\n",
    "    #'input_shape': [X_train_vl.shape[1]]\n",
    "}"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XL7ux88TXJDH",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "### Hyperparameter Optimization\n",
    "rnd_search = RandomizedSearchCV(lstm_model, params, n_iter=10, cv=3, n_jobs=1)\n",
    "print(\"X_train_vl.shape = \", X_train_vl.shape)\n",
    "print(\"y_train_vl.shape = \", y_train_vl.shape)\n",
    "rnd_search.fit(X_train_vl, y_train_vl, epochs=100,\n",
    "               validation_split=0.15,\n",
    "               callbacks=[early_stopping, checkpoint_cb])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N8L_0FBGXJDO",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "### Best validation score\n",
    "rnd_search.best_score_"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F6mzLADVXJDT",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "### Best set of hyperparameters\n",
    "best_parameters = pd.DataFrame(rnd_search.best_params_, index=['values'])\n",
    "best_parameters.to_csv(MODEL_PATH+'mlp_simple_bp.csv')\n",
    "\n",
    "rnd_search.best_params_"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yU-veC3JXJDa",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "pd.read_csv(MODEL_PATH+'mlp_simple_bp.csv', usecols=['learning_rate']).iloc[0]"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wIeJersXJDf",
    "colab_type": "text"
   },
   "source": [
    "### Training the reference model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eg9sGXc1XJDg",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "### Selecting and retraining the best model\n",
    "best = rnd_search.best_estimator_.model\n",
    "best.fit(X_train, y_train, epochs=200,\n",
    "         validation_data=(X_test, y_test),\n",
    "         callbacks=[early_stopping])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OCoyCnUwXJDm",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "### Model evaluation in accuracy terms\n",
    "best.evaluate(X_test, y_test)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90cSk1TSXJDr",
    "colab_type": "text"
   },
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qX4z601sXJDs",
    "colab_type": "text"
   },
   "source": [
    "### Checking performance improvement feeding a stream of data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KVLl5SSsXJDt",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "### Here we're going to retrain the model for each data set size.\n",
    "### ranging from 1 to len(X_train)\n",
    "\n",
    "## This variable will get the stats fo each training results\n",
    "stats = pd.DataFrame([])\n",
    "\n",
    "## The amount of data added for each training\n",
    "data_batch = 1\n",
    "number_of_steps = int(X_train.shape[0]/data_batch)\n",
    "\n",
    "## Setting the prediction data frame which will store the predictions of each trained model\n",
    "pred_columns = [(lambda x: 'size_'+str(x))(x) for x in np.arange(number_of_steps)*data_batch]\n",
    "pred_shape = np.zeros(number_of_steps*y_test.shape[0]).reshape(y_test.shape[0], number_of_steps)\n",
    "predictions = pd.DataFrame(pred_shape, columns=pred_columns)\n",
    "\n",
    "## Recreating the optimized model but with re-initialized weights in order to fit them with each data size\n",
    "config = keras.models.load_model(MODEL_PATH+'mlp_simple.h5').get_config()\n",
    "mlp_model = keras.Sequential.from_config(config)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=pd.read_csv(MODEL_PATH+'mlp_simple_bp.csv', \n",
    "                                                usecols=['learning_rate'])['learning_rate'].iloc[0])\n",
    "\n",
    "mlp_model.compile(optimizer=optimizer,\n",
    "                  metrics=['accuracy'],\n",
    "                  loss='sparse_categorical_crossentropy')\n",
    "\n",
    "init_weights = mlp_model.get_weights()\n",
    "\n",
    "## Looping through the data size and training the mdoel for each\n",
    "for i in range(1, number_of_steps):\n",
    "    mlp_model.set_weights(init_weights)\n",
    "    \n",
    "    if i*data_batch > X_train.shape[0]-1:\n",
    "        history = mlp_model.fit(X_train[:X_train.shape[0]-1,:], y_train[:y_train.shape[0]-1], epochs=100,\n",
    "                                validation_data=(X_test, y_test),\n",
    "                                callbacks=[early_stopping], workers=8)\n",
    "    \n",
    "        predictions['size_'+str(i*data_batch)] = mlp_model.predict(X_test, y_test)\n",
    "    \n",
    "        temp = pd.DataFrame(history.history)\n",
    "        stats = pd.append([stats, temp.iloc[temp.shape[0]-1,:]])\n",
    "        break\n",
    "    \n",
    "    history = mlp_model.fit(X_train[:i*data_batch,:], y_train[:i*data_batch], epochs=100,\n",
    "                            validation_data=(X_test, y_test),\n",
    "                            callbacks=[early_stopping], workers=8)\n",
    "    \n",
    "    predictions['size_'+str(i)] = np.argmax(mlp_model.predict(X_test), axis=1)\n",
    "    \n",
    "    temp = pd.DataFrame(history.history)\n",
    "    stats = stats.append(temp.iloc[temp.shape[0]-1,:])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tbPHIopUXJD0",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "### Structure of the stats data frame\n",
    "stats.head(5)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D4BFidRIXJD5",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "### Storing training stats and predictions\n",
    "stats.to_csv(META_DATA_PATH+'orig_error_analysis.csv')\n",
    "predictions.to_csv(META_DATA_PATH+'orig_error_analysis_batch'+str(data_batch)+'.csv')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qNBwB6D4XJD9",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "### Filtering only the accuracy results from the training and validation data sets\n",
    "#stats_filter = stats.iloc[stats.index.str.contains('accuracy'),:]\n",
    "\n",
    "## getting the indexes for accuracy and validation set accuracy\n",
    "#indexes = np.arange(stats_filter.shape[0]-1)\n",
    "#index_slct = indexes[indexes%2==0]\n",
    "\n",
    "## Setting the accuracy arrays for validation and training set\n",
    "#acc = stats_filter.iloc[index_slct]\n",
    "#val_acc = stats_filter.iloc[index_slct+1]"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUSJormaXJEB",
    "colab_type": "text"
   },
   "source": [
    "## Plots Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svzAQQCZXJEC",
    "colab_type": "text"
   },
   "source": [
    "### Original Data Set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "A7XSmXDAXJEE",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "### Setting plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "%matplotlib notebook"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Sr_toDzuXJEH",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "### This cell is meant to aglomerate the data into mean values of specified sized steps\n",
    "### The objective is to generate cleaner and more concise plots\n",
    "train_acc = np.array([1])\n",
    "test_acc = np.array([0])\n",
    "\n",
    "acc = np.array(stats['accuracy'])\n",
    "val_acc = np.array(stats['val_accuracy'])\n",
    "\n",
    "mean_step = 20\n",
    "\n",
    "for i in range(int(acc.shape[0]/mean_step)):\n",
    "    if (i+1)*mean_step > acc.shape[0]-1:\n",
    "            acc_mean = acc[i*mean_step:acc.shape[0]-1].mean()\n",
    "            val_acc_mean = acc[i*mean_step:val_acc.shape[0]-1].mean()\n",
    "            \n",
    "            train_acc = np.concatenate([train_acc, acc_mean], axis=0)\n",
    "            test_acc = np.concatenate([test_acc, val_acc_mean], axis=0)\n",
    "            break\n",
    "    \n",
    "    acc_mean = acc[i*mean_step:(i+1)*mean_step].mean()\n",
    "    val_acc_mean = val_acc[i*mean_step:(i+1)*mean_step].mean()\n",
    "    \n",
    "        \n",
    "    train_acc = np.append(train_acc, values=[acc_mean], axis=0)\n",
    "    test_acc = np.append(test_acc, values=[val_acc_mean], axis=0)\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OYqc7rxmXJEM",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "### Error analysis plot\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "fig.suptitle('Orignal Dataset Error Analysis', fontweight='bold', fontsize=14)\n",
    "\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "x_axis = np.arange(train_acc.shape[0])\n",
    "x_axis = mean_step*x_axis\n",
    "\n",
    "ax.plot(x_axis, 100*train_acc, color='b', label='Train Set Accuracy')\n",
    "ax.plot(x_axis, 100*test_acc, color='r', label='Test Set Accuracy')\n",
    "\n",
    "ax.set_xlim(0, x_axis[len(x_axis)-1])\n",
    "ax.set_ylim(0,100)\n",
    "\n",
    "ax.set_xlabel('Quantity of data')\n",
    "ax.set_ylabel('Accuracy %')\n",
    "ax.set_title('Model performance vs quantity of data', fontweight='bold', fontsize=12)\n",
    "\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JbXtuOSXJER",
    "colab_type": "text"
   },
   "source": [
    "#### From the graph above we can see we have a significant difference between test and training error. This means the difference in generalization error is probably due to overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AEyAB5fUXJES",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "y_test = np.array(y_test)\n",
    "jammed_idx = np.where(y_test==2)\n",
    "\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xwos3QVaXJEW",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    ""
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEy62GNqXJEb",
    "colab_type": "text"
   },
   "source": [
    "## Confusion mastrix, recall, precision and AUC ROC"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6cQ1FMDiXJEb",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "fig.savefig(IMAGES_PATH+'error_analysis_orig.png', dpi=600, bbox_inches='tight', figsize=(8,6))\n",
    "fig.savefig(IMAGES_PATH+'error_analysis_orig.jpg', dpi=600, bbox_inches='tight', figsize=(8,6))"
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}